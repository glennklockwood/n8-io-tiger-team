{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lustre Metadata Weights\n",
    "\n",
    "This notebook calculates some possible weighting factors for different Lustre metadata operations with the goal of establishing a simple metric that captures how much metadata demand different users and projects place on the storage system.  The goal is to use this metric to inform the best way to balance users across MDTs in a DNE-capable Lustre file system.\n",
    "\n",
    "The general idea is that\n",
    "\n",
    "$ M = N_{opens} * C_{opens} + N_{stats} * C_{stats} + ... $\n",
    "\n",
    "where\n",
    "\n",
    "* $M$ is a scalar value that ranks how much metadata load a user consumes\n",
    "* $N_{opens}$ is the number of file opens that users does (over a given time, for a specific job, etc)\n",
    "* $C_{opens}$ is a weighting factor that describes the relative \"cost\" of performing a file open in Lustre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import numpy\n",
    "import pandas # relies on pytables being installed too\n",
    "import matplotlib\n",
    "\n",
    "import tokio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_START = datetime.datetime(2019, 1, 1, 0, 0, 0)\n",
    "DATE_END = datetime.datetime(2019, 7, 1, 0, 0, 0)\n",
    "CACHE_FILE = 'mdrates_%s-%s.hdf5' % (DATE_START.strftime(\"%Y-%m-%d\"), DATE_END.strftime(\"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_OPS = [\n",
    "    'open',\n",
    "    'close',\n",
    "    'getattr',\n",
    "    'getxattr',\n",
    "    'link',\n",
    "    'mkdir',\n",
    "    'mknod',\n",
    "    'rename',\n",
    "    'rmdir',\n",
    "    'setattr',\n",
    "    'statfs',\n",
    "    'unlink',\n",
    "]\n",
    "NUM_OPS = len(METADATA_OPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MDS rates for each metadata operation over a period of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(CACHE_FILE):\n",
    "    mdrates_df = pandas.read_hdf(CACHE_FILE, 'mdrates')\n",
    "    print(\"Loaded mdrates (%d bytes) from %s\" % (os.path.getsize(CACHE_FILE), CACHE_FILE))\n",
    "else:\n",
    "    mdrates = {}\n",
    "    for opname in METADATA_OPS:\n",
    "        dataset_name = \"/mdtargets/%srates\" % opname\n",
    "        dataframe = tokio.tools.hdf5.get_dataframe_from_time_range(\n",
    "            fsname='cscratch',\n",
    "            datetime_start=DATE_START,\n",
    "            datetime_end=DATE_END,\n",
    "            dataset_name=dataset_name\n",
    "        )\n",
    "        # dataframe contains one column per MDS, but only the first column\n",
    "        # is populated on cscratch due to Cray LMT being DNE-incapable\n",
    "        mdrates[opname] = dataframe.iloc[:, 0]\n",
    "        print(\"Loaded %s for %s - %s\" % (dataset_name, DATE_START, DATE_END))\n",
    "        \n",
    "    mdrates_df = pandas.DataFrame.from_dict(mdrates)\n",
    "    mdrates_df.to_hdf(CACHE_FILE, 'mdrates', complevel=9)\n",
    "    print(\"Wrote mdrates (%d bytes) to %s\" % (os.path.getsize(CACHE_FILE), CACHE_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Observed metadata rates (ops/sec) for each op tracked:\\n\")\n",
    "mdrates_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpuload_file = CACHE_FILE.replace('mdrates', 'mdcpu')\n",
    "if os.path.isfile(cpuload_file):\n",
    "    cpuload_df = pandas.read_hdf(cpuload_file, 'mdcpu')\n",
    "    print(\"Loaded mdcpu (%d bytes) from %s\" % (os.path.getsize(cpuload_file), cpuload_file))\n",
    "else:\n",
    "    cpuload_df = tokio.tools.hdf5.get_dataframe_from_time_range(\n",
    "                fsname='cscratch',\n",
    "                datetime_start=DATE_START,\n",
    "                datetime_end=DATE_END,\n",
    "                dataset_name='mdservers/cpuload').iloc[:, 0]\n",
    "    cpuload_df.to_hdf(cpuload_file, 'mdcpu', complevel=9)\n",
    "    print(\"Wrote mdcpu (%d bytes) to %s\" % (os.path.getsize(cpuload_file), cpuload_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dictionary of dataframes into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, opname in enumerate(METADATA_OPS):\n",
    "    summary = mdrates_df[opname].describe()\n",
    "\n",
    "    if idx == 0:\n",
    "        print_str = \"%8s \" % \"\"\n",
    "        for metric in summary.index:\n",
    "            print_str += \"%8s \" % metric\n",
    "        print(print_str)\n",
    "        print(\"=\" * len(print_str))\n",
    "\n",
    "    print_str = \"%8s \" % opname\n",
    "    for metric in summary.index:\n",
    "        print_str += \"%8d \" % summary[metric]\n",
    "    print(print_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 1]\n",
    "while bins[-1] < 1000000000:\n",
    "    bins.append(bins[-1] * 10)\n",
    "bins.append(bins[-1] * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the distributions of metadata operations\n",
    "\n",
    "Create a histogram showing how often different metadata operations are being completed at high and low rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = matplotlib.pyplot.subplots(figsize=(12, 4))\n",
    "\n",
    "xticklabels = []\n",
    "for tickval in bins[1:]:\n",
    "    if tickval >= 1000000000:\n",
    "        xticklabels.append(\"%d B\" % (tickval / 1000000000))\n",
    "    elif tickval >= 1000000:\n",
    "        xticklabels.append(\"%d M\" % (tickval / 1000000))\n",
    "    elif tickval >= 1000:\n",
    "        xticklabels.append(\"%d K\" % (tickval / 1000))\n",
    "    else:\n",
    "        xticklabels.append(str(tickval))\n",
    "\n",
    "for idx, opname in enumerate(METADATA_OPS):         \n",
    "    counts, _ = numpy.histogram(mdrates_df[opname].dropna(), bins=bins)\n",
    "    x = range(len(counts))\n",
    "    ax.bar([x_ + 0.9 / NUM_OPS * idx for x_ in x], counts, width=0.9 / NUM_OPS, label=opname)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(xticklabels)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.set_ylabel(\"Number of samples\")\n",
    "ax.set_xlabel(\"Op rate (Hz)\")\n",
    "ax.yaxis.grid()\n",
    "ax.set_yscale('log')\n",
    "ax.set_axisbelow(True)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cumulative distribution of total metadata operations:\\n\")\n",
    "(mdrates_df.sum().sort_values(ascending=False) / mdrates_df.sum().sum()).cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrate weighting factors\n",
    "\n",
    "The `quantile` parameter below is used to decide the definition of the MDS's peak capability for a given metadata operation.  Set to more nines to get closer to the absolute maximum performance observed.\n",
    "\n",
    "The reason it is not set the max is to avoid any spuriously high measurements that may have been made during the year.\n",
    "\n",
    "Fiddling with the `quantile` parameter and observing its effect on the correlation between the resulting load metric and the MDS CPU load is a reasonable way to arrive at a good set of weights.  The underlying premise is that the CPU load on the MDS is directly tied to metadata load, and that the load metric and the CPU load are reasonable indicators of overall MDS stress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile = 0.9999\n",
    "\n",
    "quantiles = {}\n",
    "weights = {}\n",
    "min_weight = 1.0\n",
    "\n",
    "total_md = mdrates_df.sum().sum()\n",
    "\n",
    "for opname in METADATA_OPS:\n",
    "    quantiles[opname] = mdrates_df[opname].quantile(q=quantile)\n",
    "    \n",
    "    weights[opname] = 1.0 / quantiles[opname]\n",
    "    \n",
    "    pct_mdload = mdrates_df[opname].sum() / total_md\n",
    "    if pct_mdload < 0.001:\n",
    "        weights[opname] = 0.0\n",
    "        print(\"Dropping %s due to insufficient observations\" % opname)\n",
    "    else:\n",
    "        min_weight = min(weights[opname], min_weight)\n",
    "\n",
    "print()\n",
    "        \n",
    "for opname, weight in weights.items():\n",
    "    weights[opname] /= min_weight\n",
    "    \n",
    "for opname in METADATA_OPS:\n",
    "    print(\"%.4f quantile: %6d %-13s (weight=%6.1f)\" % (quantile, quantiles[opname], \"%ss/sec\" % opname, weights[opname]))\n",
    "    \n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "load_scores = mdrates_df.dot([weights[x] for x in mdrates_df.columns])\n",
    "\n",
    "# calculate correlation between cpuload and weighted sum\n",
    "print(\"Correlation coefficient between load score and MDS cpu load: %.4f\" % \n",
    "    cpuload_df.corr(load_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the ratio of CPU load to load score.  Ideally this would be a constant over time (a flat line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cpuload_df.resample('1D').sum() / load_scores.resample('1D').sum()).plot(figsize=(12, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytokio-prod",
   "language": "python",
   "name": "pytokio-prod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
